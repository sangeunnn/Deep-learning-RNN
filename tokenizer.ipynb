{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLm_Q79z8so_",
        "colab_type": "text"
      },
      "source": [
        "#Tokenizer함수\n",
        "tensorflow에 있는 token화해주는 모듈\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXTxFmELNic9",
        "colab_type": "text"
      },
      "source": [
        "#tensorflow keras의 tokenizer함수로 정수인코딩하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP8-Ya1REhrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKGvqccLNvgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0e1n4D_NzYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "877851e3-bb21-43fa-ce74-36557042d13a"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.\n",
        "print(tokenizer.word_index) #word_index를 이용해 어떻게 인코딩되었는지 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNIlJQgXN7JF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "70873d28-2a06-4d3b-8817-daac8bce8f80"
      },
      "source": [
        "print(tokenizer.word_counts) #각 단어가 카운트를 수행하였을때 몇개였는지 볼 수 있다.\n",
        "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('barber', 8),\n",
              "             ('person', 3),\n",
              "             ('good', 1),\n",
              "             ('huge', 5),\n",
              "             ('knew', 1),\n",
              "             ('secret', 6),\n",
              "             ('kept', 4),\n",
              "             ('word', 2),\n",
              "             ('keeping', 2),\n",
              "             ('driving', 1),\n",
              "             ('crazy', 1),\n",
              "             ('went', 1),\n",
              "             ('mountain', 1)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bNMx3MyOCVD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "135813a9-8a37-4d26-b8f9-f660ffd7b410"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))# texts_to_sequence // 각 단어를 인코딩된 정수 인덱스로 변환한다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpQAPL3aOEqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizer = Tokenizer(상위 n개의단어만 사용) //빈도수가 일정수 이상으로 높은 단어만 사용합니다.\n",
        "#num_words는 숫자0부터 카운트하므로 1~5까지의 수를 이용하기위해 1을 더해줌\n",
        "#0에는 단어가 없으나, 자연어처리시 패딩이라는 작업때문에 캐라스가 0까지 집합의크기로 선정함\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyTzxOEkOHvb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a7582354-a302-4bc5-ed81-aa96ae1e6400"
      },
      "source": [
        "#상위5개만 사용하려했지만 13개가 그대로출력됨\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ufk5CZsOJ5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8d86279-6fc0-40db-fdb2-30799d45c333"
      },
      "source": [
        "#실제적용은 texts_to_sequences를 이용할때 적용됩니다.\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVsRLLwmQQIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#필요하지않지만, word_index, word_count도 정해진 상위단어만 남기고싶을때 다시 fit_on_texts(sentences)해줍니다.\n",
        "tokenizer = Tokenizer() # num_words를 여기서는 지정하지 않은 상태\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZplD0NMlQSiD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9f2af4e5-e469-4628-84a0-517d68144ac7"
      },
      "source": [
        "vocab_size = 5\n",
        "words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del tokenizer.word_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "    del tokenizer.word_counts[w] # 해당 단어에 대한 카운트 정보를 삭제\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T3CqEBkQcSk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dc145ac0-22ca-49e2-ca4b-da9d515b3532"
      },
      "source": [
        "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
        "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
        "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 5],\n",
              " [1, 5],\n",
              " [1, 3, 5],\n",
              " [2],\n",
              " [2, 4, 3, 2],\n",
              " [3, 2],\n",
              " [1, 4],\n",
              " [1, 4],\n",
              " [1, 4, 2],\n",
              " [3, 2, 1],\n",
              " [1, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNWUMA9xV_ub",
        "colab_type": "text"
      },
      "source": [
        "\"OOV(Out Of Vocabulary)\"\n",
        "\n",
        "이제 word_to_index에는 빈도수가 높은 상위 5개의 단어만 저장되었습니다. \n",
        "word_to_index를 이용해서 단어들을 상위 5개의 정수로 변환합니다.\n",
        "\n",
        "sentences에서 첫번째 문장은 ['barber', 'person']이었는데, 이 문장에 대해서는 [1, 5]로 인코딩합니다. 그런데 두번째 문장인 ['barber', 'good', 'person']에는 더 이상 word_to_index에는 존재하지 않는 단어인 'good'이라는 단어가 있습니다.\n",
        "\n",
        "이처럼 단어 집합에 존재하지 않는 단어들을 Out-Of-Vocabulary(단어 집합에 없는 단어)의 약자로 'OOV'라고 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMJfAl_uQeaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 아예 단어를 제거합니다.(따로 oov라는 토큰으로바꾸지x) \n",
        "#단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 '인자' oov_token을 사용합니다.\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
        "# 빈도수 상위 5개 단어만 사용. 숫자 '0'과 'OOV'를 고려해서 단어 집합의 크기는 +2\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew2YBpVQQgMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "195270d6-e00b-474d-a7e5-8c72485105df"
      },
      "source": [
        "#만약 oov_token을 사용하기로 했다면 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 합니다.\n",
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 OOV의 인덱스 : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mssy9dFWQi5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f4dc84de-6fda-4982-d2ea-3d95c3e17b4e"
      },
      "source": [
        "#정수 인코딩완료!\n",
        "#oov까지 고려한 최종 결과\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}